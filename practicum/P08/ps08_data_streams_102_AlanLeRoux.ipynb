{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Alan Le Roux Osorio</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">alan.leroux01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">26/11/2025</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE THIS CODE AS-IS\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE THIS CODE AS-IS\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Default value for report_every\n",
    "    if report_every < 0 and max_words > 0:\n",
    "        report_every = int(max_words/5)\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "                \n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "                \n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lerou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lerou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lerou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\lerou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#ChatGPT recommended these downloads to ensure no errors as I couldn't find why it didn't work\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current noun 'able'\n",
      "Current noun 'true'\n",
      "Current noun 'child'\n",
      "Current noun 'sure'\n",
      "Current noun 'beautiful'\n",
      "Current noun 'long'\n",
      "Current noun 's'posed'\n",
      "Current noun 'white'\n",
      "Current noun 'groovy'\n",
      "Current noun 'small'\n",
      "- Read 6000/30000 words so far\n",
      "Current noun 'right'\n",
      "Current noun 'ta'\n",
      "Current noun 'sharp'\n",
      "Current noun 'fine'\n",
      "Current noun 'good'\n",
      "- Read 12000/30000 words so far\n",
      "Current noun 'last'\n",
      "Current noun 'last'\n",
      "Current noun 'hidden'\n",
      "Current noun 'secret'\n",
      "Current noun 'beautiful'\n",
      "Current noun 'new'\n",
      "Current noun 'curious'\n",
      "- Read 18000/30000 words so far\n",
      "Current noun 'evil'\n",
      "Current noun 'hard'\n",
      "Current noun 'due'\n",
      "Current noun 'subconscious'\n",
      "Current noun 'fifty'\n",
      "Current noun 'cosmic'\n",
      "- Read 24000/30000 words so far\n",
      "Current noun 'proud'\n",
      "Current noun 'other'\n",
      "Current noun 'whole'\n",
      "- Read 30000/30000 words so far\n"
     ]
    }
   ],
   "source": [
    "# MODIFY THE max_words PARAMETER TO TAKE ABOUT 30 SECONDS TO DO THIS\n",
    "\n",
    "# Maximum number of words to read, one pass should take about 30 seconds in your computer\n",
    "# 8000 wordstake 31.8 in my computer\n",
    "#30000 words took 2 minutes and 39 seconds in my computer\n",
    "MAX_WORDS = 30000\n",
    "\n",
    "for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=MAX_WORDS):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print(\"Current noun '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#the statement already gives us the parameters used in the function and a hint\n",
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    #If the reservoir is not full, add the new item\n",
    "    if len(reservoir) < max_reservoir_size:\n",
    "        reservoir.append(item)\n",
    "    else:\n",
    "        #Otherwise, we replace a random item in the reservoir with the new item\n",
    "        pos = random.randint(0, max_reservoir_size - 1)\n",
    "        reservoir[pos] = item\n",
    "    assert len(reservoir) <= max_reservoir_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Again the statement gives us the parameters used in the function\n",
    "\n",
    "def reservoir_sampling(filename, parts_of_speech, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    words_read = 0  \n",
    "    # we iterate through the words produced by read_by_parts_of_speech\n",
    "    for word in read_by_parts_of_speech(filename, parts_of_speech,\n",
    "                                        max_words=max_words,\n",
    "                                        report_every=report_every):\n",
    "        words_read += 1\n",
    "        # If the reservoir is not full, add the new word\n",
    "        if len(reservoir) < reservoir_size:\n",
    "            \n",
    "            reservoir.append(word)\n",
    "        else:\n",
    "            #otherwise, we decide whether to include the new word in the reservoir\n",
    "            #we compute the probability of including the new word   \n",
    "            p = reservoir_size / (words_read + 1)\n",
    "            #we decide randomly whether to include the new word or not\n",
    "            if random.random() < p:\n",
    "               add_to_reservoir(reservoir, word, reservoir_size)\n",
    "    #we return the number of words read and the reservoir\n",
    "    return (words_read, reservoir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 6000/30000 words so far\n",
      "- Read 12000/30000 words so far\n",
      "- Read 18000/30000 words so far\n",
      "- Read 24000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# LEAVE THIS CODE AS-IS\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_VERB], reservoir_size, max_words=MAX_WORDS)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 be\n",
      "77 have\n",
      "69 know\n",
      "66 get\n",
      "64 do\n",
      "61 go\n",
      "36 see\n",
      "36 let\n",
      "33 tell\n",
      "32 make\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:10]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most frequent items with relative frequencies:\n",
      "\n",
      " 126    8.40%  be\n",
      "  77    5.13%  have\n",
      "  69    4.60%  know\n",
      "  66    4.40%  get\n",
      "  64    4.27%  do\n",
      "  61    4.07%  go\n",
      "  36    2.40%  see\n",
      "  36    2.40%  let\n",
      "  33    2.20%  tell\n",
      "  32    2.13%  make\n",
      "  28    1.87%  think\n",
      "  26    1.73%  take\n",
      "  24    1.60%  say\n",
      "  21    1.40%  come\n",
      "  20    1.33%  believe\n",
      "  18    1.20%  want\n",
      "  18    1.20%  give\n",
      "  15    1.00%  like\n",
      "  14    0.93%  look\n",
      "  14    0.93%  find\n",
      "  13    0.87%  ask\n",
      "  12    0.80%  talk\n",
      "  12    0.80%  pay\n",
      "  12    0.80%  leave\n",
      "  12    0.80%  call\n",
      "  11    0.73%  try\n",
      "  11    0.73%  stop\n",
      "  11    0.73%  put\n",
      "  11    0.73%  keep\n",
      "  10    0.67%  mind\n"
     ]
    }
   ],
   "source": [
    "# Code to the top 30 most frequent items with relative frequencies\n",
    "#we start by computing the counts\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    if item in freq:\n",
    "        freq[item] += 1\n",
    "    else:\n",
    "        freq[item] = 1\n",
    "\n",
    "   \n",
    "#we compute the total number of items in the reservoir\n",
    "total_items = len(reservoir)\n",
    "\n",
    "# We get the  top 30 most frequent\n",
    "top30 = sorted([(count, word) for word, count in freq.items()],reverse=True)[:30]\n",
    "\n",
    "# And finally we print results with percentages\n",
    "print(\"Top 30 most frequent items with relative frequencies:\\n\")\n",
    "for count, word in top30:\n",
    "    percentage = (count / total_items) * 100\n",
    "    print(f\"{count:4d}  {percentage:6.2f}%  {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT WITH MAX_WORDS = 10000\n",
      "\n",
      "Reservoir size: 50\n",
      "- Read 2000/10000 words so far\n",
      "- Read 4000/10000 words so far\n",
      "- Read 6000/10000 words so far\n",
      "- Read 8000/10000 words so far\n",
      "- Read 10000/10000 words so far\n",
      " 10000/10000 words processed...\n",
      "\n",
      "Words read: 10005\n",
      "Top-5 words:\n",
      "\n",
      "be              | reservoir:   6 | abs est:     1200.6 | rel: 12.000%\n",
      "tell            | reservoir:   3 | abs est:      600.3 | rel:  6.000%\n",
      "have            | reservoir:   3 | abs est:      600.3 | rel:  6.000%\n",
      "know            | reservoir:   2 | abs est:      400.2 | rel:  4.000%\n",
      "get             | reservoir:   2 | abs est:      400.2 | rel:  4.000%\n",
      "\n",
      "Reservoir size: 100\n",
      "- Read 2000/10000 words so far\n",
      "- Read 4000/10000 words so far\n",
      "- Read 6000/10000 words so far\n",
      "- Read 8000/10000 words so far\n",
      "- Read 10000/10000 words so far\n",
      " 10000/10000 words processed...\n",
      "\n",
      "Words read: 10005\n",
      "Top-5 words:\n",
      "\n",
      "be              | reservoir:  10 | abs est:     1000.5 | rel: 10.000%\n",
      "get             | reservoir:   8 | abs est:      800.4 | rel:  8.000%\n",
      "have            | reservoir:   7 | abs est:      700.4 | rel:  7.000%\n",
      "take            | reservoir:   4 | abs est:      400.2 | rel:  4.000%\n",
      "tell            | reservoir:   4 | abs est:      400.2 | rel:  4.000%\n",
      "\n",
      "Reservoir size: 500\n",
      "- Read 2000/10000 words so far\n",
      "- Read 4000/10000 words so far\n",
      "- Read 6000/10000 words so far\n",
      "- Read 8000/10000 words so far\n",
      "- Read 10000/10000 words so far\n",
      " 10000/10000 words processed...\n",
      "\n",
      "Words read: 10005\n",
      "Top-5 words:\n",
      "\n",
      "be              | reservoir:  50 | abs est:     1000.5 | rel: 10.000%\n",
      "get             | reservoir:  28 | abs est:      560.3 | rel:  5.600%\n",
      "know            | reservoir:  24 | abs est:      480.2 | rel:  4.800%\n",
      "do              | reservoir:  22 | abs est:      440.2 | rel:  4.400%\n",
      "have            | reservoir:  21 | abs est:      420.2 | rel:  4.200%\n",
      "\n",
      "Reservoir size: 1000\n",
      "- Read 2000/10000 words so far\n",
      "- Read 4000/10000 words so far\n",
      "- Read 6000/10000 words so far\n",
      "- Read 8000/10000 words so far\n",
      "- Read 10000/10000 words so far\n",
      " 10000/10000 words processed...\n",
      "\n",
      "Words read: 10005\n",
      "Top-5 words:\n",
      "\n",
      "be              | reservoir:  77 | abs est:      770.4 | rel:  7.700%\n",
      "get             | reservoir:  55 | abs est:      550.3 | rel:  5.500%\n",
      "have            | reservoir:  51 | abs est:      510.3 | rel:  5.100%\n",
      "go              | reservoir:  41 | abs est:      410.2 | rel:  4.100%\n",
      "know            | reservoir:  39 | abs est:      390.2 | rel:  3.900%\n",
      "\n",
      "Reservoir size: 5000\n",
      "- Read 2000/10000 words so far\n",
      "- Read 4000/10000 words so far\n",
      "- Read 6000/10000 words so far\n",
      "- Read 8000/10000 words so far\n",
      "- Read 10000/10000 words so far\n",
      " 10000/10000 words processed...\n",
      "\n",
      "Words read: 10005\n",
      "Top-5 words:\n",
      "\n",
      "be              | reservoir: 475 | abs est:      950.5 | rel:  9.500%\n",
      "do              | reservoir: 272 | abs est:      544.3 | rel:  5.440%\n",
      "get             | reservoir: 242 | abs est:      484.2 | rel:  4.840%\n",
      "know            | reservoir: 225 | abs est:      450.2 | rel:  4.500%\n",
      "have            | reservoir: 211 | abs est:      422.2 | rel:  4.220%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We now run experiments with different reservoir sizes\n",
    "MAX_WORDS = 10000\n",
    "\n",
    "reservoir_sizes = [50, 100, 500, 1000, 5000]\n",
    "\n",
    "print(\"EXPERIMENT WITH MAX_WORDS =\", MAX_WORDS)\n",
    "print()\n",
    "#for each reservoir size, we perform reservoir sampling and compute estimates\n",
    "for R in reservoir_sizes:\n",
    "    print(f\"Reservoir size: {R}\")\n",
    "    reservoir = []\n",
    "    seen = 0\n",
    "    # We perform reservoir sampling for verbs\n",
    "    for w in read_by_parts_of_speech(INPUT_FILE, [POS_VERB], max_words=MAX_WORDS):\n",
    "        seen += 1\n",
    "\n",
    "        # indicator for progress (not rlly necessary)\n",
    "        if seen % 10000 == 0:\n",
    "            print(f\" {seen}/{MAX_WORDS} words processed...\")\n",
    "        # Reservoir sampling logic\n",
    "        if len(reservoir) < R:\n",
    "            reservoir.append(w)\n",
    "        else:\n",
    "            p = R / seen\n",
    "            if random.random() < p:\n",
    "                reservoir[random.randint(0, R - 1)] = w\n",
    "    # Now we compute frequencies in the reservoir\n",
    "    freq = {}\n",
    "    for w in reservoir:\n",
    "        freq[w] = freq.get(w, 0) + 1\n",
    "    # We get the top-5 words in the reservoir\n",
    "    top5 = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    print(f\"\\nWords read: {seen}\")\n",
    "    print(\"Top-5 words:\\n\")\n",
    "    # For each of the top-5 words, we compute absolute and relative estimates\n",
    "    for word, c in top5:\n",
    "        est_abs = c * seen / R\n",
    "        est_rel = est_abs / seen * 100\n",
    "        print(f\"{word:15s} | reservoir: {c:3d} | abs est: {est_abs:10.1f} | rel: {est_rel:6.3f}%\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiments, it’s clear that the reservoir size strongly affects the accuracy of the estimated word frequencies. With very small reservoirs (50–100), the results were unstable and the most frequent words changed a lot, which shows that the sample was too small to represent the full dataset. As the reservoir size increased (500–1000), the estimates became more consistent and the top words appeared reliably. The largest reservoir (5000) produced the most stable and realistic frequencies, closely matching what we would expect from the full data(although it is also the most expensive in memory.)\n",
    "\n",
    "If I had to choose a practical size, I would recommend around 1000, since it provides good accuracy without being too heavy. But if memory is not a concern, a larger reservoir like 5000 gives the best approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE THIS CODE AS-IS\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEAVE THIS CODE AS-IS\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 1: 2048 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 2: 16384 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 3: 64 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 4: 1024 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 5: 1024 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 6: 4096 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 7: 512 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 8: 1024 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 9: 2048 distinct words\n",
      "- Read 1000/5000 words so far\n",
      "- Read 2000/5000 words so far\n",
      "- Read 3000/5000 words so far\n",
      "- Read 4000/5000 words so far\n",
      "- Read 5000/5000 words so far\n",
      "Estimate on pass 10: 32768 distinct words\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 5000 #I decided to change maxwords becouse otherwise it took too long with 10 passes (the next cell was > 10 hours)\n",
    "number_of_passes = 10\n",
    "estimates = []\n",
    "# We perform multiple passes to get multiple estimates\n",
    "for i in range(number_of_passes):\n",
    "    # For each pass, we create a new random hash function\n",
    "    hash_function = random_hash_function()\n",
    "    R = 0\n",
    "    # We read the words and compute the maximum number of trailing zeroes\n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_VERB], max_words=MAX_WORDS):\n",
    "        h = abs(hash_function(word))\n",
    "        if h == 0:\n",
    "            r = 0\n",
    "        else:\n",
    "            r = count_trailing_zeroes(h)\n",
    "        if r > R:\n",
    "            R = r\n",
    "    # We compute the estimate based on the maximum number of trailing zeroes\n",
    "    estimate = 2 ** R\n",
    "    #And we store the estimate\n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i + 1, estimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 6099.2\n",
      "* Median  of estimates: 1536.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns — Run 1, Pass 1: Estimate = 16384\n",
      "Nouns — Run 1, Pass 2: Estimate = 8192\n",
      "Nouns — Run 1, Pass 3: Estimate = 32768\n",
      "Nouns — Run 1, Pass 4: Estimate = 32768\n",
      "Nouns — Run 1, Pass 5: Estimate = 32768\n",
      "Nouns — Run 1, Pass 6: Estimate = 32768\n",
      "Nouns — Run 1, Pass 7: Estimate = 8192\n",
      "Nouns — Run 1, Pass 8: Estimate = 16384\n",
      "Nouns — Run 1, Pass 9: Estimate = 4096\n",
      "Nouns — Run 1, Pass 10: Estimate = 16384\n",
      "Nouns — Run 1: Average estimate over 10 passes = 20070.4\n",
      "Nouns — Run 2, Pass 1: Estimate = 131072\n",
      "Nouns — Run 2, Pass 2: Estimate = 65536\n",
      "Nouns — Run 2, Pass 3: Estimate = 32768\n",
      "Nouns — Run 2, Pass 4: Estimate = 8192\n",
      "Nouns — Run 2, Pass 5: Estimate = 16384\n",
      "Nouns — Run 2, Pass 6: Estimate = 4096\n",
      "Nouns — Run 2, Pass 7: Estimate = 32768\n",
      "Nouns — Run 2, Pass 8: Estimate = 1048576\n",
      "Nouns — Run 2, Pass 9: Estimate = 32768\n",
      "Nouns — Run 2, Pass 10: Estimate = 32768\n",
      "Nouns — Run 2: Average estimate over 10 passes = 140492.8\n",
      "Nouns — Run 3, Pass 1: Estimate = 16384\n",
      "Nouns — Run 3, Pass 2: Estimate = 32768\n",
      "Nouns — Run 3, Pass 3: Estimate = 32768\n",
      "Nouns — Run 3, Pass 4: Estimate = 16384\n",
      "Nouns — Run 3, Pass 5: Estimate = 32768\n",
      "Nouns — Run 3, Pass 6: Estimate = 4096\n",
      "Nouns — Run 3, Pass 7: Estimate = 8192\n",
      "Nouns — Run 3, Pass 8: Estimate = 262144\n",
      "Nouns — Run 3, Pass 9: Estimate = 8192\n",
      "Nouns — Run 3, Pass 10: Estimate = 65536\n",
      "Nouns — Run 3: Average estimate over 10 passes = 47923.2\n",
      "Nouns: Median of average estimates across 3 runs = 47923.2\n",
      "\n",
      "Adjectives — Run 1, Pass 1: Estimate = 16384\n",
      "Adjectives — Run 1, Pass 2: Estimate = 65536\n",
      "Adjectives — Run 1, Pass 3: Estimate = 8192\n",
      "Adjectives — Run 1, Pass 4: Estimate = 131072\n",
      "Adjectives — Run 1, Pass 5: Estimate = 8192\n",
      "Adjectives — Run 1, Pass 6: Estimate = 131072\n",
      "Adjectives — Run 1, Pass 7: Estimate = 131072\n",
      "Adjectives — Run 1, Pass 8: Estimate = 16384\n",
      "Adjectives — Run 1, Pass 9: Estimate = 8192\n",
      "Adjectives — Run 1, Pass 10: Estimate = 4096\n",
      "Adjectives — Run 1: Average estimate over 10 passes = 52019.2\n",
      "Adjectives — Run 2, Pass 1: Estimate = 32768\n",
      "Adjectives — Run 2, Pass 2: Estimate = 32768\n",
      "Adjectives — Run 2, Pass 3: Estimate = 4096\n",
      "Adjectives — Run 2, Pass 4: Estimate = 8192\n",
      "Adjectives — Run 2, Pass 5: Estimate = 4096\n",
      "Adjectives — Run 2, Pass 6: Estimate = 4096\n",
      "Adjectives — Run 2, Pass 7: Estimate = 65536\n",
      "Adjectives — Run 2, Pass 8: Estimate = 8192\n",
      "Adjectives — Run 2, Pass 9: Estimate = 131072\n",
      "Adjectives — Run 2, Pass 10: Estimate = 2048\n",
      "Adjectives — Run 2: Average estimate over 10 passes = 29286.4\n",
      "Adjectives — Run 3, Pass 1: Estimate = 131072\n",
      "Adjectives — Run 3, Pass 2: Estimate = 4096\n",
      "Adjectives — Run 3, Pass 3: Estimate = 8192\n",
      "Adjectives — Run 3, Pass 4: Estimate = 16384\n",
      "Adjectives — Run 3, Pass 5: Estimate = 16384\n",
      "Adjectives — Run 3, Pass 6: Estimate = 8192\n",
      "Adjectives — Run 3, Pass 7: Estimate = 32768\n",
      "Adjectives — Run 3, Pass 8: Estimate = 16384\n",
      "Adjectives — Run 3, Pass 9: Estimate = 8192\n",
      "Adjectives — Run 3, Pass 10: Estimate = 16384\n",
      "Adjectives — Run 3: Average estimate over 10 passes = 25804.8\n",
      "Adjectives: Median of average estimates across 3 runs = 29286.4\n",
      "\n",
      "Verbs — Run 1, Pass 1: Estimate = 1024\n",
      "Verbs — Run 1, Pass 2: Estimate = 8192\n",
      "Verbs — Run 1, Pass 3: Estimate = 2048\n",
      "Verbs — Run 1, Pass 4: Estimate = 131072\n",
      "Verbs — Run 1, Pass 5: Estimate = 8192\n",
      "Verbs — Run 1, Pass 6: Estimate = 4096\n",
      "Verbs — Run 1, Pass 7: Estimate = 8192\n",
      "Verbs — Run 1, Pass 8: Estimate = 4096\n",
      "Verbs — Run 1, Pass 9: Estimate = 8192\n",
      "Verbs — Run 1, Pass 10: Estimate = 2048\n",
      "Verbs — Run 1: Average estimate over 10 passes = 17715.2\n",
      "Verbs — Run 2, Pass 1: Estimate = 4096\n",
      "Verbs — Run 2, Pass 2: Estimate = 4096\n",
      "Verbs — Run 2, Pass 3: Estimate = 4096\n",
      "Verbs — Run 2, Pass 4: Estimate = 4096\n",
      "Verbs — Run 2, Pass 5: Estimate = 16384\n",
      "Verbs — Run 2, Pass 6: Estimate = 8192\n",
      "Verbs — Run 2, Pass 7: Estimate = 4096\n",
      "Verbs — Run 2, Pass 8: Estimate = 4096\n",
      "Verbs — Run 2, Pass 9: Estimate = 16384\n",
      "Verbs — Run 2, Pass 10: Estimate = 8192\n",
      "Verbs — Run 2: Average estimate over 10 passes = 7372.8\n",
      "Verbs — Run 3, Pass 1: Estimate = 16384\n",
      "Verbs — Run 3, Pass 2: Estimate = 8192\n",
      "Verbs — Run 3, Pass 3: Estimate = 8192\n",
      "Verbs — Run 3, Pass 4: Estimate = 65536\n",
      "Verbs — Run 3, Pass 5: Estimate = 4096\n",
      "Verbs — Run 3, Pass 6: Estimate = 4096\n",
      "Verbs — Run 3, Pass 7: Estimate = 2048\n",
      "Verbs — Run 3, Pass 8: Estimate = 16384\n",
      "Verbs — Run 3, Pass 9: Estimate = 2048\n",
      "Verbs — Run 3, Pass 10: Estimate = 1024\n",
      "Verbs — Run 3: Average estimate over 10 passes = 12800.0\n",
      "Verbs: Median of average estimates across 3 runs = 12800.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#with max_words = 5000 this cell took around 424 minutes to run\n",
    "number_of_runs = 3\n",
    "passes_per_run = 10\n",
    "\n",
    "#We will store the results for each part of speech here\n",
    "POS_TOTAL = {\"Nouns\": [POS_NOUN], \"Adjectives\": [POS_ADJECTIVE], \"Verbs\": [POS_VERB]}\n",
    "\n",
    "results = {}\n",
    "#We perform the experiments for each part of speech\n",
    "for pos_name, pos_list in POS_TOTAL.items():\n",
    "    run_averages = []\n",
    "    #We perform multiple runs to get more reliable estimates\n",
    "    for run in range(number_of_runs):\n",
    "        estimates = []\n",
    "        #For each run, we perform multiple passes\n",
    "        for p in range(passes_per_run):\n",
    "            h = random_hash_function()\n",
    "            R = 0\n",
    "            # We read the words and compute the maximum number of trailing zeroes\n",
    "            for word in read_by_parts_of_speech(INPUT_FILE, pos_list, max_words=-1, report_every=-1):\n",
    "                hv = h(word)\n",
    "                tz = count_trailing_zeroes(abs(hv))\n",
    "                R = max(R, tz)\n",
    "            # We compute the estimate based on the maximum number of trailing zeroes\n",
    "            estimate = 2 ** R\n",
    "            estimates.append(estimate)\n",
    "            print(f\"{pos_name} — Run {run+1}, Pass {p+1}: Estimate = {estimate}\")\n",
    "        # We compute the average estimate for this run\n",
    "        avg_estimate = sum(estimates) / len(estimates)\n",
    "        run_averages.append(avg_estimate)\n",
    "        print(f\"{pos_name} — Run {run+1}: Average estimate over 10 passes = {avg_estimate}\")\n",
    "    # We compute the median of the average estimates across runs\n",
    "    median_estimate = statistics.median(run_averages)\n",
    "    results[pos_name] = median_estimate\n",
    "    print(f\"{pos_name}: Median of average estimates across 3 runs = {median_estimate}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that I completed this practice myself, that my answers were not written by an AI-enabled code assistant, and that except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
